recommenderRegistry$get_entries(dataType = "realRatingMatrix")
library(recommenderlab)
library(ggplot2)
#Problem Understanding
#create small artificial dataset
m <- matrix(sample(c(as.numeric(0:5), NA), 50,
replace=TRUE, prob=c(rep(.4/6,6),.6)), ncol=10,
dimnames = list(user=paste("u", 1:5, sep=''),
item=paste("i", 1:10, sep='')))
#coerce M to realRatingMatrix object which stores the data
#in sparse format
r <- as(m, "realRatingMatrix")
r
as(r, "list")
as(r, "data.frame")
#Normalize rating matrix entries
#e.g. remove rating bias by subtracting the row mean fromm
#all ratings in the row.
r_m <- normalize(r)
#Visually inspect small portions of rating matrix
image(r, main = "Raw_Ratings")
image(r_m, main = "Normalized_Ratings")
#Which ratings are above a certain threshold
r_b <- binarize(r, minRating = 4)
as(r_b, "matrix")
#####################
#Data Understanind and Preparation
#We will use 100k ratings from 943 users on 1664 movies
#collected from sept 1997 - april 1998 through the
#movie lens website
#Load the data
data("MovieLense")
MovieLense
#Visualize a sample of this
image(MovieLense, main = "Raw_ratings")
summary(getRatings(MovieLense))
#It appears some movies were not rated at all by first few users
#Visualizing ratings
qplot(getRatings(MovieLense), binwidth=1,
main="Histogram_of_ratings", xlab="Rating")
#The rating plot looks skewed to the right
#Normalization
qplot(getRatings(normalize(MovieLense, method = "Z-score")),
main="Histogram_of_normalized_raings", xlab="Rating")
summary(getRatings(normalize(MovieLense, method="Z-score")))
#Seems better, normalization will take care of user bias
#How many movies did people rate on average?
qplot(rowCounts(MovieLense), binwidth=10,
main="Movies_rated_on_average",
xlab = "#_of_users",
ylab = "#_of_movies_rated")
#Some people get tired of rating movies at a logarithmic pace.
#What is the mean rating of each movie?
qplot(colMeans(MovieLense), binwidth=.1,
main = "Mean_rating_of_movies",
xlab = "Rating",
ylab = "#_of_movies")
#The big spike on 1 suggest that this could be interpreted as binary.
#In other words, some people don't want to see certain movies at all
#Same on 5 and 3
##################
#Modeling
recommenderRegistry$get_entries(dataType = "realRatingMatrix")
scheme <- evaluationScheme(MovieLense, method="split", train=.9,
k=1, given=10, goodRating=4)
scheme
algorithms <- list(
"random_items" = list(name="RANDOM", param=list(normalize="Z-score")),
"popular_items" = list(name="POPULAR", param=list(normalize="Z-score")),
"user_based_CF" = list(name="UBCF", param=list(normalize="Z-score",
method="Cosine",
nn=50, minRating=3)),
"item_based_CF" = list(name="IBCF", param=list(normalize="Z-score"))
)
results <- evaluate(scheme, algorithms, n=c(1,3,5,10,15,20))
plot(results, annotat 1:4, legend="topleft")
plot(results, annotate=1:4, legend="topleft")
plot(results, "prec/rec", annotate=3)
scheme2 <- evaluationScheme(MovieLense, method="cross", k=4, given=3,
goodRating=5)
scheme2
results2  <- evaluate(scheme2, method="POPULAR", n=c(1,3,5,10,15,20))
results2
getConfusionMatrix(results2)[[1]]
avg(getConfusionMatrix(results2)[[1]])
library(rstudioapi)
crime.data <- read.csv("crime.data.csv")
memory.limit()
memory.limit(size=16000)
m
mean_r = function(x) {
m = 0
n = length(x)
for(i in seq_len(n))
m = m + x[i]/n
m
}
install.packages("compiler")
library(compiler)
cmp_mean_r = cmpfun(mean_r)
x = rnorm(1000)
install.packages("microbenchmark")
library(microbenchmark)
microbenchmark(times=10, unit="ms",
mean_r(x), cmp_mean_r(x), mean(x))
install.packages("bigmemory")
library(bigmemory)
install.packages("sqldf")
library(sqldf)
write.csv(iris, "iris.csv", quote=FALSE, row.names = FALSE)
iris2 <- read.csv.sql("iris.csv",
sql="selec_*_from_file_where_Species_=_'setosa'_")
iris2 <- read.csv.sql("iris.csv",
sql="select_*_from_file_where_Species_=_'setosa'_")
iris2 <- read.csv.sql("iris.csv",
sql="select * from file where Species = 'setosa' ")
library(compiler)
install.packages("hflights")
library(hflights)
write.csv(hflights, 'hflights.csv', row.names = FALSE)
DecJFKflights <- read.csv.sql('hflights.csv',
sql="select * from file where Dest='\"JFK\"' and Month = 12)
DecJFKflights <- read.csv.sql('hflights.csv',
sql="select * from file where Dest='\"JFK\"' and Month = 12')
DecJFKflights <- read.csv.sql('hflights.csv',
sql="select * from file where Dest='\"JFK\"' and Month = 12")
View(DecJFKflights)
memory.limit(size=24000)
memory.limit(size=16000)
library(compiler)
install.packages("RMySQL")
library(RMySQL)
conn <- dbConnect(MySQL(), dbname="cities", username="root",
password="")
library(RMySQL)
conn <- dbConnect(MySQL(), dbname="cities", username="root",
password="")
dbListTables(conn)
dbListFields(conn, "villes_france_free")
cities <- dbGetQuery(conn, "SELECT * FROM 'villes_france_free'")
cities <- dbGetQuery(conn, "SELECT * FROM 'villes_france_free")
cities <- dbGetQuery(conn, "SELECT * FROM 'villes_france_free'")
conn <- dbConnect(MySQL(), dbname="cities", username="root",
password="")
cities <- dbGetQuery(conn, "SELECT * FROM 'villes_france_free'")
cities <- dbGetQuery(conn, "selet * from 'villes_france_free'")
cities <- dbGetQuery(conn, "select * from 'villes_france_free'")
'
cities <- dbGetQuery(conn, "SELECT * FROM 'villes_france_free'")
cities <- dbGetQuery(conn, "SELECT * FROM 'villes_france_free'")
cities <- dbGetQuery(conn, "SELECT * FROM villes_france_free")
View(cities)
summary(cities)
head(cities)
features    <- read.csv('C:/Users/schwi/Google Drive/Data Projects/Dengue Prediction/Github/dengue_features_train.csv')
labels      <- read.csv('C:/Users/schwi/Google Drive/Data Projects/Dengue Prediction/Github/dengue_labels_train.csv')
submission  <- read.csv('C:/Users/schwi/Google Drive/Data Projects/Dengue Prediction/Github/dengue_features_test.csv')
View(features)
View(labels)
features$total_cases <- labels$total_cases
features$city==sj
features$city=='sj'
sj <- features[features$city=='sj',]
iq <- features[features$city=='iq',]
summary(features)
features <- features[complete.cases(features),]
View(sj)
sj <- features[features$city=='sj',]
iq <- features[features$city=='iq',]
View(sj)
sj[,'previous'] <- NA
if(2==2 && 1==1) {i=2}
if(2==2 &&
1==1) {i=2}
if(2==2 &&
1==1) {i=2}
if(2==
2 &&
1==1) {i=2}
if(2==
2 &&
1==1) {i=3}
if(features[i,c('year','city')]==features[i-1,c('year','city')] &&
}
if(2==
2 &&
1==1) {i=3}
#Break into 2 datasets by location
sj <- features[features$city=='sj',]
iq <- features[features$city=='iq',]
features$weekofyear[1]
if(2==
2 &
1==1) {i=3}
if(2==
2 |
1==1) {i=4}
as.date(features$week_start_date[36])
features$week_start_date[36]
features$week_start_date[[36]]
features$week_start_date
features$week_start_date[1]
features$week_start_date[2]
features$week_start_date[36]
features$week_start_date[45]
rownames(features) <- NULL
features$week_start_date[30]-features$week_start_date[29]
as.DATE(features$week_start_date[30])-as.Date(features$week_start_date[29])
as.Date(features$week_start_date[30])-as.Date(features$week_start_date[29])
as.Date(features$week_start_date[31])-as.Date(features$week_start_date[30])
as.Date(features$week_start_date[32])-as.Date(features$week_start_date[31])
(as.Date(features$week_start_date[32])-as.Date(features$week_start_date[31]))<10
features    <- read.csv('C:/Users/schwi/Google Drive/Data Projects/Dengue Prediction/Github/dengue_features_train.csv')
labels      <- read.csv('C:/Users/schwi/Google Drive/Data Projects/Dengue Prediction/Github/dengue_labels_train.csv')
submission  <- read.csv('C:/Users/schwi/Google Drive/Data Projects/Dengue Prediction/Github/dengue_features_test.csv')
features$total_cases <- labels$total_cases
features <- features[complete.cases(features),]
rownames(features) <- NULL
features[,'previous'] <- NA
for(i in 2:nrow(features))
{
if(features$city[i]==features$city[i-1] &
((as.Date(features$week_start_date[i])
-as.Date(features$week_start_date[31]))<10))
{
features$previous[i] <- TRUE
}
}
summary(features)
features[,'previous'] <- NA
for(i in 2:nrow(features))
{
if(features$city[i]==features$city[i-1] &
((as.Date(features$week_start_date[i])
-as.Date(features$week_start_date[i-1]))<10))
{
features$previous[i] <- TRUE
}
}
summary(features)
colnames(features)
colnames(features[,5:ncol(features)])
colnames(features[,5:ncol(features)-2])
a <- colnames(features[,5:ncol(features)-2])
a[1]
names <- colnames(features[,5:ncol(features)-2])
nrow(names)
length(names)
features[,ncol(features):(ncol(features)+length(names))] <- NA
for(i in 1:length(names))
{
name[i] <- paste(name[i],'prev',sep="")
}
for(i in 1:length(names))
{
names[i] <- paste(names[i],'prev',sep="")
}
names
names <- colnames(features[,5:ncol(features)-2])
features[,ncol(features):(ncol(features)+length(names))] <- NA
for(i in 1:length(names))
{
names[i] <- paste(names[i],'prev',sep="_")
}
colname(features[,ncol(features):(ncol(features)+length(names))]) <- names
features    <- read.csv('C:/Users/schwi/Google Drive/Data Projects/Dengue Prediction/Github/dengue_features_train.csv')
features[,'previous'] <- NA
names <- colnames(features[,5:ncol(features)-2])
features[,ncol(features):(ncol(features)+length(names))] <- NA
for(i in 1:length(names))
{
names[i] <- paste(names[i],'prev',sep="_")
}
colname(features[,ncol(features):(ncol(features)+length(names))]) <- names
colname(features[,ncol(features):(ncol(features)+length(names))])
colnames(features[,ncol(features):(ncol(features)+length(names))]) <- names
<- names
colnames(features[,ncol(features):(ncol(features)+length(names))])
features    <- read.csv('C:/Users/schwi/Google Drive/Data Projects/Dengue Prediction/Github/dengue_features_train.csv')
features    <- read.csv('C:/Users/schwi/Google Drive/Data Projects/Dengue Prediction/Github/dengue_features_train.csv')
features[,'previous'] <- NA
names <- colnames(features[,5:ncol(features)-2])
start <- ncol(features)
features[,ncol(features):(ncol(features)+length(names))] <- NA
for(i in 1:length(names))
{
names[i] <- paste(names[i],'prev',sep="_")
}
colnames(features[,start:start+length(names))]) <- names
colnames(features[,start:(start+length(names))]) <- names
colnames(features[,start:(start+length(names))]) <- names
colnames(features[,start:(start+length(names))])
names
colnames(features[,start+1:(start+length(names))]) <- names
start <- ncol(features)+1
colnames(features[,start:(start+length(names))])
features    <- read.csv('C:/Users/schwi/Google Drive/Data Projects/Dengue Prediction/Github/dengue_features_train.csv')
#Add previous week's data as new feature
features[,'previous'] <- NA
#prepare names for the new columns
names <- colnames(features[,5:ncol(features)-2])
start <- ncol(features)
features[,ncol(features):(ncol(features)+length(names))] <- NA
for(i in 1:length(names))
{
names[i] <- paste(names[i],'prev',sep="_")
}
colnames(features[,start+1:(start+length(names))]) <- names
colnames(features[,start+1:(start+length(names))])
colnames(features[,(start+1):(start+length(names))]) <- names
colnames(features[,(start+1):(start+length(names))])
names
colnames(features[,(start+1):(start+length(names))]) <- names
colnames(features[,(start+1):(start+length(names))])
colnames(features[,(start+1):(start+length(names))]) <- c(names)
colnames(features[,(start+1):(start+length(names))])
colnames(features[,(start+1):(start+length(names))],c(names))
features[,c(names)] <- NA
names <- colnames(features[,5:ncol(features)-2])
for(i in 1:length(names))
{
names[i] <- paste(names[i],'prev',sep="_")
}
features[,c(names)] <- NA
features    <- read.csv('C:/Users/schwi/Google Drive/Data Projects/Dengue Prediction/Github/dengue_features_train.csv')
features[,'previous'] <- NA
#prepare new columns and names
names <- colnames(features[,5:ncol(features)-2])
for(i in 1:length(names))
{
names[i] <- paste(names[i],'prev',sep="_")
}
features[,c(names)] <- NA
check <- features[,5:27]
View(check)
View(check)
check <- features[,5:25]
check <- features[,5:24]
View(check)
features    <- read.csv('C:/Users/schwi/Google Drive/Data Projects/Dengue Prediction/Github/dengue_features_train.csv')
library(rstudioapi)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
features    <- read.csv('dengue_features_train.csv')
names <- colnames(features[,5:ncol(features)-2])
for(i in 1:length(names))
{
names[i] <- paste(names[i],'prev',sep="_")
}
features[,c(names)] <- NA
for(i in 2:nrow(features))
{
if(features$city[i]==features$city[i-1] &
((as.Date(features$week_start_date[i])
-as.Date(features$week_start_date[i-1]))<10))
{
features[i,25:34] <- features[i-1,5:24]
}
}
features[i,25:44] <- features[i-1,5:24]
for(i in 2:nrow(features))
{
if(features$city[i]==features$city[i-1] &
((as.Date(features$week_start_date[i])
-as.Date(features$week_start_date[i-1]))<10))
{
features[i,25:44] <- features[i-1,5:24]
}
}
features    <- read.csv('dengue_features_train.csv')
View(features)
View(features)
names <- colnames(features[,5:ncol(features)-2])
for(i in 1:length(names))
{
names[i] <- paste(names[i],'prev',sep="_")
}
features[,c(names)] <- NA
for(i in 2:nrow(features))
{
if(features$city[i]==features$city[i-1] &
((as.Date(features$week_start_date[i])
-as.Date(features$week_start_date[i-1]))<10))
{
features[i,25:44] <- features[i-1,5:24]
}
}
#Combine features and labels
features$total_cases <- labels$total_cases
#remove rows with missing data
features <- features[complete.cases(features),]
rownames(features) <- NULL
#looks like we lose about 250 observations
#Break into 2 datasets by location
sj <- features[features$city=='sj',]
iq <- features[features$city=='iq',]
